{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Processament i visualització LAION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97f1e3df7499775c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import tqdm\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T11:20:16.131136922Z",
     "start_time": "2023-12-20T11:20:15.013171687Z"
    }
   },
   "id": "490afe0e2c30ddc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scrapping the XAC database\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "webdriver.firefox.marionette=False\n",
    "# El rows = 1000 travieso és per aprofitar el bug\n",
    "BASE_ARXIU = 'https://arxiusenlinia.cultura.gencat.cat'\n",
    "QUERY_GLOBAL = BASE_ARXIU +'/#/cercaavancada/cerca'\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "option.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "#For ChromeDriver version 79.0.3945.16 or over\n",
    "option.add_argument('--disable-blink-features=AutomationControlled')\n",
    "\n",
    "option.add_argument(\"window-size=1280,800\")\n",
    "option.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\")\n",
    "\n",
    "DRIVERPATH = 'geckodriver' \n",
    "OUPATH = 'arxiu/'\n",
    "DRIVER = webdriver.Chrome(options=option)\n",
    "DRIVER.get(QUERY_GLOBAL)\n",
    "\n",
    "DRIVER.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "\n",
    "def check_if_loading():\n",
    "    try:\n",
    "        return DRIVER.find_element(By.CLASS_NAME, 'spinner-container').is_displayed\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "\n",
    "def scrap():\n",
    "    while True:\n",
    "        # while check_if_loading(): pass\n",
    "    \n",
    "        for element in BeautifulSoup(DRIVER.page_source, features=\"html.parser\").find('table').find_all(class_='row'):\n",
    "            soup_elelment = element # METADATA FROM HERE\n",
    "            photo = soup_elelment.find('img', class_ = 'image-list-item-responsive')\n",
    "            if photo is not None:\n",
    "                if photo['src'] in visited: continue # Cal fer-ho en dos nivells perque si és None no és iterable per tant no té \"in\" \n",
    "            desc = soup_elelment.find_all(class_ = 'contingut-nom-metadada')\n",
    "            if not len(desc): continue\n",
    "            try:\n",
    "                date = desc[2].find_next_siblings('span')[0]\n",
    "            except IndexError:\n",
    "                date = \"<span>1/1/0000</span>\" # Placeholder\n",
    "            desc = desc[-1].find_next_siblings('span')\n",
    "            title = soup_elelment.find(class_ = 'titol-text-resultats')\n",
    "            if photo is not None and title is not None and desc is not None:\n",
    "                photo = photo['src']\n",
    "                title = title\n",
    "                desc = desc[0]\n",
    "                date = date\n",
    "    \n",
    "                data = {\n",
    "                    'image': str(photo),\n",
    "                    'caption': str(title).replace('\\n', ' '),\n",
    "                    'desc': str(desc).replace('\\n', ' '),\n",
    "                    'date': str(date)\n",
    "                }\n",
    "                visited.add(photo)\n",
    "                \n",
    "                CSV.write(\"\\n\" + '\\t'.join(list(data.values()))) # Separat per tabulacions per evitar problemes de parsing amb les commes de les fotos i descripcions\n",
    "        succeed = False\n",
    "        while not succeed:\n",
    "            # De vegades el botó de següent está oclòs ja que BS4 va més ràpid que selenium carregant les coses, per algun motiu comprobar el display no funciona\n",
    "            try:\n",
    "                next = WebDriverWait(DRIVER, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, 'ui-paginator-next'))\n",
    "                )\n",
    "                next.click()\n",
    "                succeed = True\n",
    "            except: pass\n",
    "            \n",
    "\n",
    "input(\"Press enter to proceed... \") # Cal inserir la cerca manualment perque funciona desde front-end; tirem de selenium\n",
    "CSV = open('imatges2.tsv', 'w')\n",
    "CSV.write('\\t'.join(['url', 'caption', 'desc', 'date']))\n",
    "visited = set()\n",
    "scrap()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95bcb83ce7183037"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Functions to download images from XAC (previously scrapped) and resize them\n",
    "\n",
    "def download_images(data, data_location, check_unique=True, train_test_split=0.8):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        data: File name with the url, captions and description of the images\n",
    "        data_location: Directory name where the data is stored\n",
    "        check_unique: Bool to check unique captions (deletes duplicate captions)\n",
    "        train_test_split: Ratio of images for train\n",
    "\n",
    "    \"\"\"\n",
    "    train_dir = os.path.join(data_location, 'train')\n",
    "    test_dir = os.path.join(data_location, 'test')\n",
    "\n",
    "    # Create dataframe for images\n",
    "    images_df = pd.read_csv(os.path.join(data_location, data), sep='\\t', encoding=\"ISO-8859-1\")\n",
    "\n",
    "    # Create captions files\n",
    "    with open(os.path.join(data_location, \"captions_train_raw.tsv\"), \"w\") as f:\n",
    "        f.write(\"image\\tcaption\\n\")\n",
    "\n",
    "    with open(os.path.join(data_location, \"captions_test_raw.tsv\"), \"w\") as f:\n",
    "        f.write(\"image\\tcaption\\n\")\n",
    "\n",
    "    id = 0\n",
    "    unique_captions = set()\n",
    "    if check_unique:\n",
    "        # Download images from url and fill captions file\n",
    "        for url, caption in zip(images_df.url.values, images_df.caption.values):\n",
    "            name = os.path.split(url)[1]\n",
    "            caption = caption[35:-7]\n",
    "            if caption in unique_captions:\n",
    "                id += 1\n",
    "                continue\n",
    "            unique_captions.add(caption)\n",
    "            r = requests.get(url, stream=True)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                i = Image.open(io.BytesIO(r.content))\n",
    "\n",
    "                if np.random.rand() < train_test_split:\n",
    "                    i.save(os.path.join(train_dir, name), quality=95)\n",
    "                    with open(os.path.join(data_location, \"captions_train_raw.tsv\"), \"a\") as f:\n",
    "                        f.write(name + \"\\t\" + caption + \"\\n\")\n",
    "                else:\n",
    "                    i.save(os.path.join(test_dir, name), quality=95)\n",
    "                    with open(os.path.join(data_location, \"captions_test_raw.tsv\"), \"a\") as f:\n",
    "                        f.write(name + \"\\t\" + caption + \"\\n\")\n",
    "\n",
    "            if (id % 100) == 0:\n",
    "                print(\n",
    "                    f\"Progress: {id}/{len(images_df.url.values)} with {len(unique_captions)} unique captions so far.\\n\")\n",
    "            id += 1\n",
    "    else:\n",
    "        # Download images from url and fill captions file\n",
    "        for url, caption in zip(images_df.url.values, images_df.caption.values):\n",
    "            name = os.path.split(url)[1]\n",
    "            caption = caption[35:-7]\n",
    "            r = requests.get(url, stream=True)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                i = Image.open(io.BytesIO(r.content))\n",
    "\n",
    "                if np.random.rand() < train_test_split:\n",
    "                    i.save(os.path.join(train_dir, name), quality=95)\n",
    "                    with open(os.path.join(data_location, \"captions_train_raw.tsv\"), \"a\") as f:\n",
    "                        f.write(name + \"\\t\" + caption + \"\\n\")\n",
    "                else:\n",
    "                    i.save(os.path.join(test_dir, name), quality=95)\n",
    "                    with open(os.path.join(data_location, \"captions_test_raw.tsv\"), \"a\") as f:\n",
    "                        f.write(name + \"\\t\" + caption + \"\\n\")\n",
    "\n",
    "            if (id % 100) == 0:\n",
    "                print(f\"Progress: {id}/{len(images_df.url.values)} \\n\")\n",
    "            id += 1\n",
    "    print('Finished downloading all images')\n",
    "\n",
    "\n",
    "def resize_images(image_dir, output_dir, size):\n",
    "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    for i, image in enumerate(images):\n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = img.resize(size, Image.Resampling.LANCZOS)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                  .format(i + 1, num_images, output_dir))\n",
    "\n",
    "\n",
    "def resize_images_dew(data, output_dir, root_dir, size):\n",
    "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if type(data) == tuple:\n",
    "        data_0 = pd.read_csv(data[0], header=None, sep=',')\n",
    "        data_1 = pd.read_csv(data[1], header=None, sep=',')\n",
    "        data = pd.concat([data_0, data_1])\n",
    "        del data_0, data_1\n",
    "    else:\n",
    "        data = pd.read_csv(data, header=None, sep=',')\n",
    "    names = data[1]\n",
    "\n",
    "    num_images = len(names)\n",
    "    for i, image in enumerate(names):\n",
    "        image = str(image)\n",
    "        with open(os.path.join(root_dir, image[:1], image[1:3], image + '.jpg'), 'rb') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = img.resize(size, Image.Resampling.LANCZOS)\n",
    "                img.save(os.path.join(output_dir, image + '.jpg'), img.format)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                  .format(i + 1, num_images, output_dir))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69921e221747fac6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Distribució d'idiomes del subset sencer\n",
    "\n",
    "filename = \"part-00000-fc82da14-99c9-4ff6-ab6a-ac853ac82819-c000.snappy.parquet\"\n",
    "print(pq.ParquetFile(filename).schema)\n",
    "df = pq.read_table(filename).to_pandas()\n",
    "\n",
    "langs = dict()\n",
    "for el in df['LANGUAGE'].values:\n",
    "    print(el)\n",
    "    if el not in langs.keys():\n",
    "        langs[el] = 1\n",
    "    else:\n",
    "        langs[el] += 1\n",
    "\n",
    "pprint(langs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc7425fd21bcc353"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Netejar train original per incloure només imatges decarregades\n",
    "\n",
    "df = pd.read_csv('/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/laion/train.csv', sep='\\t')\n",
    "imgs = os.listdir('/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/laion/img_size/')\n",
    "\n",
    "imgs = set(imgs)\n",
    "langs = dict()\n",
    "rmv = list()\n",
    "\n",
    "with tqdm.tqdm(total=len(imgs)) as pbar:\n",
    "    for i, id, lang in zip(df.index, df['SAMPLE_ID'], df['LANGUAGE']):\n",
    "        if str(id) + '.jpg' in imgs:\n",
    "            if lang not in langs.keys():\n",
    "                langs[lang] = 1\n",
    "            else:\n",
    "                langs[lang] += 1\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            rmv.append(i)\n",
    "\n",
    "pprint(langs)\n",
    "\n",
    "df = df.drop(rmv)\n",
    "df.to_csv('clean_train.csv', sep='\\t')\n",
    "print(len(df))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aad0c06ce5de2d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Canviar format caption\n",
    "\n",
    "file_path = '/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/laion/captions.jsonl'\n",
    "caps = pd.read_json(path_or_buf=file_path, lines=True)\n",
    "\n",
    "langs = caps.columns[2:]\n",
    "caps_2 = list()\n",
    "\n",
    "for i in caps.index:\n",
    "    for lang in langs:\n",
    "        row = {'image/key': caps['image/key'].iloc[i], 'image/locale': caps['image/locale'].iloc[i],\n",
    "               'caption': caps[lang].iloc[i]['caption'][0], 'lang': lang}\n",
    "        caps_2.append(row)\n",
    "        if len(caps[lang].iloc[i]['caption']) > 1:\n",
    "            row = {'image/key': caps['image/key'].iloc[i], 'image/locale': caps['image/locale'].iloc[i],\n",
    "                   'caption': caps[lang].iloc[i]['caption'][1], 'lang': lang}\n",
    "            caps_2.append(row)\n",
    "\n",
    "df = pd.DataFrame(caps_2)\n",
    "\n",
    "df.to_csv('captions.tsv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5695cadcb639ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Anàlisi de les coincidències entre etiquetatges de llenguatges i similarity\n",
    "\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "\n",
    "\n",
    "file_path = 'final_train.csv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "langs = np.unique(df['LANGUAGE'])\n",
    "print(len(langs))\n",
    "unknown = ['co', 'fy', 'gd', 'ha', 'iw', 'mi', 'my', 'sd', 'sm', 'sn', 'so', 'st', 'su', 'tg', 'uz', 'yi', 'yo']\n",
    "ids = [i for i, x in enumerate(langs) if x in unknown]\n",
    "langs = np.delete(langs, ids)\n",
    "langs = np.append(langs, 'en')\n",
    "print(langs)\n",
    "\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "identifier.set_languages(langs)\n",
    "print(identifier.classify(\"This is a test\")[0])\n",
    "\n",
    "\n",
    "similarity27 = df[df['similarity'] > 0.27]\n",
    "similarity28 = df[df['similarity'] > 0.28]\n",
    "similarity29 = df[df['similarity'] > 0.29]\n",
    "similarity30 = df[df['similarity'] > 0.3]\n",
    "\n",
    "print('0.27:', len(similarity27),\n",
    "      similarity27['TEXT'].head(30),\n",
    "      '\\n0.28:', len(similarity28),\n",
    "      similarity28['TEXT'].head(30),\n",
    "      '\\n0.29:', len(similarity29),\n",
    "      similarity29['TEXT'].head(30),\n",
    "      '\\n0.30:', len(similarity30),\n",
    "      similarity30['TEXT'].head(30))\n",
    "\n",
    "preds = [identifier.classify(text)[0] for text in df['TEXT']]\n",
    "origi = df['LANGUAGE']\n",
    "cnt_ori = dict(Counter(origi))\n",
    "cnt_ori['en'] = 0\n",
    "\n",
    "cnt_prd = dict(Counter(preds))\n",
    "for lang in unknown:\n",
    "    cnt_prd[lang] = 0\n",
    "\n",
    "cnt_prd = dict(sorted(cnt_prd.items(), key=lambda x: x[1], reverse=True))\n",
    "cnt_ori = dict(sorted(cnt_ori.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "df_prd = pd.DataFrame(cnt_prd.items(), columns=['language', 'count'])\n",
    "df_ori = pd.DataFrame(cnt_ori.items(), columns=['language', 'count'])\n",
    "\n",
    "en_pred = [pred == 'en' for pred in preds]\n",
    "accuracy = [pred == org for pred, org in zip(preds, origi)]\n",
    "print(\"Percentatge d'anglès:\", sum(en_pred)/len(en_pred)*100,\n",
    "      \"Precisió respecte l'original:\", sum(accuracy)/len(accuracy)*100)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x=\"language\", y=\"count\", data=df_ori,\n",
    "            label=\"Original\", color=\"b\")\n",
    "\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x=\"language\", y=\"count\", data=df_prd,\n",
    "            alpha=0.5, label=\"Predicted\", color=\"r\")\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(title=\"Language labels accuracy\", ylabel=\"Count\", xlabel=\"Languages\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "\n",
    "f.savefig('comparison.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0f47da8f9132b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Anàlisi de les coincidències entre etiquetatges de llenguatges i similarity\n",
    "\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = 'final_train.csv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "langs = np.unique(df['LANGUAGE'])\n",
    "print(len(langs))\n",
    "unknown = ['co', 'fy', 'gd', 'ha', 'iw', 'mi', 'my', 'sd', 'sm', 'sn', 'so', 'st', 'su', 'tg', 'uz', 'yi', 'yo']\n",
    "ids = [i for i, x in enumerate(langs) if x in unknown]\n",
    "langs = np.delete(langs, ids)\n",
    "langs = np.append(langs, 'en')\n",
    "print(langs)\n",
    "all_langs = np.append(langs, unknown)\n",
    "\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "identifier.set_languages(langs)\n",
    "print(identifier.classify(\"This is a test\")[0])\n",
    "\n",
    "preds = [identifier.classify(text)[0] for text in df['TEXT']]\n",
    "origi = df['LANGUAGE']\n",
    "similarity = np.array(df['similarity'])\n",
    "\n",
    "idx_preds = defaultdict(list)\n",
    "for i, lang in enumerate(preds):\n",
    "    idx_preds[lang].append(i)\n",
    "\n",
    "idx_origi = defaultdict(list)\n",
    "for i, lang in enumerate(origi):\n",
    "    idx_origi[lang].append(i)\n",
    "\n",
    "mean_preds = {lang: np.mean(similarity[i]) for lang, i in idx_preds.items()}\n",
    "mean_origi = {lang: np.mean(similarity[i]) for lang, i in idx_origi.items()}\n",
    "\n",
    "df_prd = pd.DataFrame(mean_preds.items(), columns=['language', 'clip'])\n",
    "df_ori = pd.DataFrame(mean_origi.items(), columns=['language', 'clip'])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(17, 8))\n",
    "\n",
    "# Plot the total crashes\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.catplot(data=df_ori, x='language', y='clip',\n",
    "            label=\"Original\", color=\"b\")\n",
    "\n",
    "# Plot the crashes where alcohol was involved\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.catplot(data=df_prd, x='language', y='clip',\n",
    "            alpha=0.8, label=\"Predicted\", color=\"r\")\n",
    "\n",
    "# Add a legend and informative axis label\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n",
    "ax.set(title=\"Language labels similarity\", ylabel=\"CLIP similarity mean\", xlabel=\"Languages\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "f.savefig('similarity.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45d18bf1be13eab3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confusion matrix llenguatges\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "df = pd.read_csv('preds.csv', sep='\\t')\n",
    "labels = np.unique(df['prediccio'])\n",
    "matplotlib.rcParams['figure.figsize'] = 19, 15\n",
    "cm = confusion_matrix(df['original'], df['prediccio'], labels=labels)\n",
    "\n",
    "magma = matplotlib.colormaps['magma'].reversed()\n",
    "cmap = ListedColormap(magma(np.linspace(0.1, 1, 256)))\n",
    "cmap.set_bad('white')      # color of mask on heatmap\n",
    "cmap.set_under('white')    # color of mask on cbar\n",
    "\n",
    "sns.heatmap(cm,\n",
    "    cmap=cmap, vmin=5,            # set cbar range from 0.5 to 1\n",
    "    mask=cm < 5,                  # use \"bad\" color for thresholded values\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cbar_kws={'extend': 'min'})     # extend cbar to show \"under\" color\n",
    "\n",
    "plt.savefig('confusionmatrix.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f947d798b869a03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confusion matrix percentatges\n",
    "\n",
    "df = pd.read_csv('preds.csv', sep='\\t')\n",
    "total = pd.read_csv('/data2fast/users/esanchez/laion/clean_train.csv', sep='\\t')\n",
    "\n",
    "labels_pred = set(np.unique(df['prediccio']))\n",
    "labels_origi = np.unique(df['original'])\n",
    "labels_pred.update(labels_origi)\n",
    "labels = sorted(list(labels_pred))\n",
    "matplotlib.rcParams['figure.figsize'] = 19, 15\n",
    "cm = confusion_matrix(df['original'], df['prediccio'], labels=labels)\n",
    "cm = np.array([(row / np.sum(row)) if np.sum(row) > 0 else row for row in cm])\n",
    "\n",
    "\n",
    "threshold = 0\n",
    "del_list = list()\n",
    "for i, row in enumerate(cm):\n",
    "    if row[i] < threshold:\n",
    "        del_list.append(i)\n",
    "\n",
    "del_list = list(reversed(del_list))\n",
    "cm = np.delete(cm, del_list, 0)\n",
    "cm = np.delete(cm, del_list, 1)\n",
    "labels = np.delete(labels, del_list)\n",
    "\n",
    "magma = matplotlib.colormaps['magma'].reversed()\n",
    "cmap = ListedColormap(magma(np.linspace(0.1, 1, 256)))\n",
    "cmap.set_bad('white')      # color of mask on heatmap\n",
    "cmap.set_under('white')    # color of mask on cbar\n",
    "\n",
    "sns.heatmap(cm,\n",
    "    cmap=cmap, vmin=0.0001,            # set cbar range from 0.5 to 1\n",
    "    mask=cm == 0,                  # use \"bad\" color for thresholded values\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cbar_kws={'extend': 'min'})     # extend cbar to show \"under\" color\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('Laion labels')\n",
    "plt.savefig('confusionmatrix_0.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b7499a1a4776b77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Crear dataset de coincidències\n",
    "\n",
    "df = pd.read_csv('preds.csv', sep='\\t')\n",
    "total = pd.read_csv('/data2fast/users/esanchez/laion/clean_train.csv', sep='\\t')\n",
    "df = df[df['prediccio'] == df['original']]\n",
    "\n",
    "total = total.iloc[df.index]\n",
    "print(total)\n",
    "\n",
    "names = total.columns\n",
    "unnamed = [name for name in names if name[:3] == 'Unn']\n",
    "total = total.drop(columns=unnamed)\n",
    "total.to_csv('train_coincidences.csv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cacd90922f0fc5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fer recompte de les llengües\n",
    "\n",
    "df = pd.read_csv('train_coincidences.csv', sep='\\t')\n",
    "cnt = dict(Counter(df['LANGUAGE']))\n",
    "\n",
    "cnt = dict(sorted(cnt.items(), key=lambda x: x[0]))\n",
    "df_cnt = pd.DataFrame(cnt.items(), columns=['language', 'count'])\n",
    "\n",
    "df_cnt.to_csv('count.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b83e560c101d60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('count.csv')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_theme(rc={'figure.figsize': (15, 8)})\n",
    "\n",
    "ax = sns.barplot(df, x=\"language\", y=\"count\", errorbar=None)\n",
    "ax.bar_label(ax.containers[0], fontsize=10, rotation=90)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n",
    "ax.set(ylim=(0, 1.6e6), ylabel=\"Amount of samples\", xlabel=\"Language codes\")\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('laion_count.png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d1e4c93130a1e44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Crear arxiu scenes del dataset DeBoer\n",
    "\n",
    "path = '/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/deboer/scene_detection_set'\n",
    "classes = next(os.walk(path))[1]\n",
    "dirs = [os.path.join(path, fold) for fold in classes]\n",
    "\n",
    "numberOfRows = len([[name for name in os.listdir(fold) if os.path.isfile(name)] for fold in dirs])\n",
    "print(numberOfRows)\n",
    "\n",
    "# dsp anar afegint les dades per carpetes (escenes)\n",
    "df = pd.DataFrame(index=np.arange(0, numberOfRows), columns=['file', 'scene'])\n",
    "i = 0\n",
    "for name in classes:\n",
    "    dir_path = os.path.join(path, name)\n",
    "    new = [{'file': file, 'scene': name} for file in os.listdir(dir_path) if os.path.isfile(file)]\n",
    "    df.iloc[i:i+len(new)] = new\n",
    "    i += len(new)\n",
    "\n",
    "df.to_csv('scenes.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b48d87d8aec56980"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Separar dataset en train i test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/deboer/scenes.csv\")\n",
    "df = df[['file', 'scene']]\n",
    "\n",
    "train = df.copy()\n",
    "test = None\n",
    "scenes = np.unique(df.scene)\n",
    "del_rows = list()\n",
    "\n",
    "split = 0.05\n",
    "for scene in scenes:\n",
    "    total = np.sum(df.scene == scene)\n",
    "    n_test = int(total * split)\n",
    "\n",
    "    idx = np.random.choice(total, size=n_test, replace=False)\n",
    "    rows = df[df.scene == scene].iloc[idx]\n",
    "\n",
    "    if test is None:\n",
    "        test = pd.DataFrame(rows)\n",
    "    else:\n",
    "        test = pd.concat([test, rows], ignore_index=True)\n",
    "    del_rows.extend(rows.index)\n",
    "\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "train.drop(del_rows, inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "test.to_csv(\"/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/deboer/test.csv\")\n",
    "train.to_csv(\"/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/deboer/train.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d27d11c28e95d510"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Separar dataset en train i test\n",
    "\n",
    "split = 0.05\n",
    "\n",
    "df = pd.read_csv('captions.tsv', sep='\\t')\n",
    "total = len(df)\n",
    "\n",
    "idx = np.sort(np.random.choice(total, int(total*split), replace=False))\n",
    "\n",
    "test = df.iloc[idx]\n",
    "\n",
    "idx_neg = [i not in idx for i in range(total)]\n",
    "\n",
    "train = df[idx_neg]\n",
    "\n",
    "train.to_csv('train.tsv', sep='\\t', index=False)\n",
    "test.to_csv('test.tsv', sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3c6a0c6c6996f4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Afegir NER a dades LAION (es, de, nl)\n",
    "\n",
    "import pandas as pd\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def space_before(caption, tokens):\n",
    "    j = 1\n",
    "    space = list()\n",
    "    caption = caption[j:]\n",
    "    for idx, token in enumerate(tokens[1:]):\n",
    "        j = caption.find(token.text)\n",
    "        space.append(caption[j-1] == \" \")\n",
    "        j += len(token.text)\n",
    "        caption = caption[j:]\n",
    "    space.append(False)\n",
    "    return space\n",
    "\n",
    "\n",
    "def ner_sentence(caption, tagger):\n",
    "    sent = Sentence(caption)\n",
    "    tagger.predict(sent)\n",
    "\n",
    "    space = space_before(caption, sent.tokens)\n",
    "\n",
    "    span = list()\n",
    "    labels = list()\n",
    "    for label in sent.get_labels():\n",
    "        aux = label.unlabeled_identifier.split(\"[\")[1].split(\":\")\n",
    "        ll = [int(aux[0])]\n",
    "        aux = aux[1].split(\"]\")\n",
    "        ll.append(int(aux[0]))\n",
    "        span.append(ll)\n",
    "        labels.append(label.value)\n",
    "\n",
    "    span.reverse()\n",
    "    labels.reverse()\n",
    "\n",
    "    caption = [t.text for t in sent.tokens]\n",
    "    aux = list()\n",
    "    for ran, label in zip(span, labels):\n",
    "        caption[ran[0]] = token[label]\n",
    "        caption[ran[0] + 1:ran[1]] = ''\n",
    "        if ran[1] - ran[0] > 1:\n",
    "            aux.extend(range(ran[1] - 1, ran[0], -1))\n",
    "\n",
    "    for elem in aux:\n",
    "        try:\n",
    "            space.pop(elem)\n",
    "        except IndexError:\n",
    "            print(\"caption\", caption)\n",
    "            print(\"labels\", sent.get_labels())\n",
    "            print(\"space\", space)\n",
    "            print(\"aux\", aux)\n",
    "            print(\"unlabeled_identifier\", sent.unlabeled_identifier)\n",
    "            input()\n",
    "    out = \"\".join([f\"{t} \" if b else t for t, b in zip(caption, space)])\n",
    "    return out\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/run/user/1001/gvfs/sftp:host=158.109.8.116,port=22345,user=esanchez/data2fast/users/esanchez/laion/train_coincidences.csv\", sep='\\t')\n",
    "df = df[df['LANGUAGE'] == 'es']\n",
    "\n",
    "tagger = SequenceTagger.load(\"flair/ner-multi-fast\")\n",
    "\n",
    "token = {'LOC': '<loc>', 'ORG': '<org>', 'PER': '<per>', 'MISC': '<misc>'}\n",
    "\n",
    "handler = open('train_ner.tsv', 'w')\n",
    "handler.write(\"SAMPLE_ID\\tURL\\tTEXT\\tHEIGHT\\tWIDTH\\tLICENSE\\tLANGUAGE\\tNSFW\\tsimilarity\\n\")\n",
    "\n",
    "for data in tqdm(df.values):\n",
    "    _, sample_id, url, text, height, width, license, language, nsfw, similarity = data\n",
    "    caption = ner_sentence(text, tagger)\n",
    "    handler.write(f\"{sample_id}\\t{url}\\t{caption}\\t{height}\\t{width}\\t{license}\\t{language}\\t{nsfw}\\t{similarity}\\n\")\n",
    "\n",
    "handler.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4691b13b4ebc3f39"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Afegir NER a dades LAION (fr, it)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 'cuda:3'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True, device=3)\n",
    "\n",
    "token = {'LOC': '<loc>', 'ORG': '<org>', 'PER': '<per>', 'MISC': '<misc>'}\n",
    "\n",
    "def masked_string(example, nlp):\n",
    "    results = nlp(example)\n",
    "    ll = [*example]\n",
    "    shift = 0\n",
    "    for mask in results:\n",
    "        aux = ll[mask['end'] + shift:]\n",
    "        ll[mask['start'] + shift:mask['start'] + len(token[mask['entity_group']]) + shift] = [*token[mask['entity_group']]]\n",
    "        ll = ll[:mask['start'] + len(token[mask['entity_group']]) + shift] + aux\n",
    "        shift = (mask['start'] + len(token[mask['entity_group']])) - mask['end'] + shift\n",
    "    return ''.join(ll)\n",
    "\n",
    "df = pd.read_csv(\"/data2fast/users/esanchez/laion/train_coincidences.csv\", sep='\\t')\n",
    "df = df[(df['LANGUAGE'] == 'fr') | (df['LANGUAGE'] == 'it')]\n",
    "\n",
    "handler = open('/data2fast/users/esanchez/laion/train_ner_fr_it.tsv', 'w')\n",
    "handler.write(\"SAMPLE_ID\\tURL\\tTEXT\\tHEIGHT\\tWIDTH\\tLICENSE\\tLANGUAGE\\tNSFW\\tsimilarity\\n\")\n",
    "\n",
    "for data in tqdm(df.values):\n",
    "    _, sample_id, url, text, height, width, license, language, nsfw, similarity = data\n",
    "    caption = masked_string(text, nlp)\n",
    "    handler.write(f\"{sample_id}\\t{url}\\t{caption}\\t{height}\\t{width}\\t{license}\\t{language}\\t{nsfw}\\t{similarity}\\n\")\n",
    "\n",
    "handler.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc865f53ee3eb6f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "xac-ic",
   "language": "python",
   "display_name": "xac-ic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
